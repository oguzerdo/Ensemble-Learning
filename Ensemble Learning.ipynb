{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, VotingClassifier\n",
    "\n",
    "# warning library\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Data Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 42\n",
    "\n",
    "n_samples = 200\n",
    "n_features = 10\n",
    "n_classes = 2\n",
    "n_estimators = 10\n",
    "noise_class = 0.1\n",
    "noise_moon = 0.1\n",
    "noise_circle = 0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = make_classification(n_samples = n_samples,\n",
    "                          n_features = n_features,\n",
    "                          n_classes = n_classes,\n",
    "                          n_repeated = 0,\n",
    "                          n_redundant = 0,\n",
    "                          n_informative = n_features - 1,\n",
    "                          random_state = random_state,\n",
    "                          n_clusters_per_class = 1,\n",
    "                          flip_y = noise_class)\n",
    "data = pd.DataFrame(x)\n",
    "data[\"target\"] = y\n",
    "plt.figure()\n",
    "sns.scatterplot(x = data.iloc[:,0], y = data.iloc[:,1], hue = \"target\", data = data);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset for multiclass. And it will be called \" Dataset # 2 \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_classification = (x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moon dataset\n",
    "moon = make_moons(n_samples = n_samples,\n",
    "                  noise = noise_moon,\n",
    "                  random_state = random_state)\n",
    "\n",
    "data_moon = pd.DataFrame(moon[0])\n",
    "data_moon[\"target\"] = moon[1]\n",
    "plt.figure\n",
    "sns.scatterplot(x = data_moon.iloc[:,0], y = data_moon.iloc[:,1], hue = \"target\", data = data_moon);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset for binary classification.And it will be called \" Dataset # 0 \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Circle dataset\n",
    "circle = make_circles(n_samples = n_samples,\n",
    "                  noise = noise_circle,\n",
    "                  factor = 0.1,\n",
    "                  random_state = random_state)\n",
    "data_circle = pd.DataFrame(circle[0])\n",
    "data_circle[\"target\"] = circle[1]\n",
    "plt.figure\n",
    "sns.scatterplot(x = data_circle.iloc[:,0], y = data_circle.iloc[:,1], hue = \"target\", data = data_circle);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset for binary classification.And it will be called \" Dataset # 1 \"\n",
    "\n",
    "In order to be able to visualize in the future, we will have to combine data sets;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [moon, circle]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are combining these two data sets as we will make binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN, SVM, Decision Tree // Random Forest, AdaptiveBoosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC()\n",
    "knn = KNeighborsClassifier(n_neighbors=15)\n",
    "dt = DecisionTreeClassifier(random_state = random_state)\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators = n_estimators, random_state = random_state)\n",
    "ada = AdaBoostClassifier(base_estimator = dt, n_estimators = n_estimators, random_state = random_state)\n",
    "v1 = VotingClassifier(estimators = [(\"svc\",svc),(\"KNN\",knn),(\"Decision Tree\",dt),(\"Random Forest\",rf),(\"AdaBoost\",ada)])\n",
    "\n",
    "\n",
    "classifiers = [svc, knn, dt, rf, ada, v1]\n",
    "names = [\"SVC\", \"KNN\", \"Decision Tree\",\"Random Forest\",\"AdaBoost\",\"Voting Classifier\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset # 0\n",
      "SVC: test set score: 0.7825 \n",
      "SVC: train set score: 0.7958333333333333 \n",
      "\n",
      "KNN: test set score: 0.77125 \n",
      "KNN: train set score: 0.8158333333333333 \n",
      "\n",
      "Decision Tree: test set score: 0.78875 \n",
      "Decision Tree: train set score: 0.8108333333333333 \n",
      "\n",
      "Random Forest: test set score: 0.785 \n",
      "Random Forest: train set score: 0.8016666666666666 \n",
      "\n",
      "AdaBoost: test set score: 0.785 \n",
      "AdaBoost: train set score: 0.8116666666666666 \n",
      "\n",
      "Voting Classifier: test set score: 0.7825 \n",
      "Voting Classifier: train set score: 0.8091666666666667 \n",
      "\n",
      "-------------------------------------\n",
      "Dataset # 1\n",
      "SVC: test set score: 0.66 \n",
      "SVC: train set score: 0.6933333333333334 \n",
      "\n",
      "KNN: test set score: 0.64875 \n",
      "KNN: train set score: 0.7266666666666667 \n",
      "\n",
      "Decision Tree: test set score: 0.58875 \n",
      "Decision Tree: train set score: 0.6241666666666666 \n",
      "\n",
      "Random Forest: test set score: 0.6475 \n",
      "Random Forest: train set score: 0.705 \n",
      "\n",
      "AdaBoost: test set score: 0.665 \n",
      "AdaBoost: train set score: 0.73 \n",
      "\n",
      "Voting Classifier: test set score: 0.65125 \n",
      "Voting Classifier: train set score: 0.7141666666666666 \n",
      "\n",
      "-------------------------------------\n",
      "Dataset # 2\n",
      "SVC: test set score: 0.5925 \n",
      "SVC: train set score: 0.6733333333333333 \n",
      "\n",
      "KNN: test set score: 0.555 \n",
      "KNN: train set score: 0.6533333333333333 \n",
      "\n",
      "Decision Tree: test set score: 0.585 \n",
      "Decision Tree: train set score: 0.6108333333333333 \n",
      "\n",
      "Random Forest: test set score: 0.58875 \n",
      "Random Forest: train set score: 0.6241666666666666 \n",
      "\n",
      "AdaBoost: test set score: 0.55 \n",
      "AdaBoost: train set score: 0.6758333333333333 \n",
      "\n",
      "Voting Classifier: test set score: 0.58375 \n",
      "Voting Classifier: train set score: 0.6475 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "%matplotlib qt\n",
    "h = 0.2\n",
    "i = 1\n",
    "figure = plt.figure(figsize=(18, 6))\n",
    "for ds_cnt, ds in enumerate(datasets):\n",
    "    # preprocess dataset, split into training and test part\n",
    "    X, y = ds\n",
    "    X = RobustScaler().fit_transform(X)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.4, random_state=random_state)\n",
    "\n",
    "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "\n",
    "    cm = plt.cm.RdBu\n",
    "    cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
    "\n",
    "    ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n",
    "    \n",
    "    if ds_cnt == 0:\n",
    "        ax.set_title(\"Input data\")\n",
    "        \n",
    "    ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,edgecolors='k')\n",
    "    ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.6,marker = '^', edgecolors='k')\n",
    "    \n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "    i += 1\n",
    "\n",
    "    print(\"Dataset # {}\".format(ds_cnt))\n",
    "\n",
    "    for name, clf in zip(names, classifiers):\n",
    "        \n",
    "        ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n",
    "        \n",
    "        clf.fit(X_train, y_train)\n",
    "        \n",
    "        score = clf.score(X_test, y_test)\n",
    "        \n",
    "        print(\"{}: test set score: {} \".format(name, score))\n",
    "        \n",
    "        score_train = clf.score(X_train, y_train)  \n",
    "        \n",
    "        print(\"{}: train set score: {} \".format(name, score_train))\n",
    "        print()\n",
    "        \n",
    "        \n",
    "        if hasattr(clf, \"decision_function\"):\n",
    "            Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "        else:\n",
    "            Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "        # Put the result into a color plot\n",
    "        Z = Z.reshape(xx.shape)\n",
    "        ax.contourf(xx, yy, Z, cmap=cm, alpha=.8)\n",
    "\n",
    "        # Plot the training points\n",
    "        ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,\n",
    "                   edgecolors='k')\n",
    "        # Plot the testing points\n",
    "        ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright,marker = '^',\n",
    "                   edgecolors='white', alpha=0.6)\n",
    "\n",
    "        ax.set_xticks(())\n",
    "        ax.set_yticks(())\n",
    "        if ds_cnt == 0:\n",
    "            ax.set_title(name)\n",
    "        score = score*100\n",
    "        ax.text(xx.max() - .3, yy.min() + .3, ('%.1f' % score),\n",
    "                size=15, horizontalalignment='right')\n",
    "        i += 1\n",
    "    print(\"-------------------------------------\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "def make_classify(dc, clf, name):\n",
    "    x, y = dc\n",
    "    x = RobustScaler().fit_transform(x)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=.4, random_state=random_state)\n",
    "    \n",
    "    for name, clf in zip(names, classifiers):\n",
    "        clf.fit(X_train, y_train)\n",
    "        score = clf.score(X_test, y_test)\n",
    "        print(\"{}: test set score: {} \".format(name, score))\n",
    "        score_train = clf.score(X_train, y_train)  \n",
    "        print(\"{}: train set score: {} \".format(name, score_train))\n",
    "        print()\n",
    "\n",
    "print(\"Dataset # 2\")   \n",
    "make_classify(data_classification, classifiers,names)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SUMMARY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Our models work well as there is a distinction between our classes.\n",
    "- Increasing the feature number makes the SVM model better.\n",
    "- SVM performance decreases if noise increases\n",
    "- If the number of classes increases, KNN will give good results.\n",
    "- If the number of features increases, SVM will give good results.\n",
    "- When the number of samples reaches 20000 30000, it may make sense to turn to a neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM\n",
    "\n",
    "- Difficult to understand, not an easy-to-learn model\n",
    "- It is not easy to interpret the results\n",
    "- If the number of feature is set correctly, it can be successful despite high noise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN\n",
    "- Very simple algorithm\n",
    "- Does not require training\n",
    "- As the size of the data increases, the decision-making time in KNN may take longer\n",
    "- For training : time efficient | for test : no time efficient\n",
    "- Affected by outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Between KNN AND SVM\n",
    "\n",
    "- If dimension ( number of feature) is increasing : choose SVM\n",
    "- If number of class is increasing : choose KNN\n",
    "- SVM for binary classification\n",
    "- KNN for multiclass problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree\n",
    "- The biggest problem is overfitting\n",
    "For fix this:\n",
    "1) Regularization\n",
    "2) Random Forest\n",
    "- Very effective in simple data sets\n",
    "- But very affected by outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "- Consists of trees\n",
    "- Doing random forest with decision tree when overfitting has reduced overfitting a bit \n",
    "- Feature importance estimation is really good"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------\n",
    "- n_samples = 200\n",
    "- n_features = 10\n",
    "- n_classes = 2\n",
    "- n_estimators = 10\n",
    "- noise_moon = 0.1 noise_circle = 0.1 noise_class = 0.2\n",
    "\n",
    "------------------------------\n",
    "\n",
    "## Results :\n",
    "\n",
    "<b>Data Set # 0 ( Moon )</b>\n",
    "- Decision Tree: test set score: 0.975 \n",
    "- Decision Tree: train set score: 1.0 \n",
    "\n",
    "\n",
    "- Random Forest: test set score: 0.95 \n",
    "- Random Forest: train set score: 0.9916666666666667 \n",
    "\n",
    "<b>Data Set # 1 ( Circle )</b>\n",
    "- Decision Tree: test set score: 0.9875 \n",
    "- Decision Tree: train set score: 1.0 \n",
    "\n",
    "\n",
    "- Random Forest: test set score: 1.0 \n",
    "- Random Forest: train set score: 1.0 \n",
    "\n",
    "<b>Data Set # 2 ( Classification )  #There is overfitting problem </b>\n",
    "- Decision Tree: test set score: 0.65 \n",
    "- Decision Tree: train set score: 1.0 \n",
    "\n",
    "\n",
    "- Random Forest: test set score: 0.75 \n",
    "- Random Forest: train set score: 0.9666666666666667 \n",
    "\n",
    "<b> (In Data Set # 2 Overfitting decreased a little when random forest was made on the decision tree )</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How can we fix overfitting problem ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets increase the number of samples 200 to 2000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset # 0\n",
    "\n",
    "- Decision Tree: test set score: 0.9875 \n",
    "- Decision Tree: train set score: 1.0 \n",
    "\n",
    "\n",
    "- Random Forest: test set score: 0.99875 \n",
    "- Random Forest: train set score: 1.0 \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Dataset # 1\n",
    "\n",
    "- Decision Tree: test set score: 0.99625 \n",
    "- Decision Tree: train set score: 1.0 \n",
    "\n",
    "\n",
    "- Random Forest: test set score: 0.99125 \n",
    "- Random Forest: train set score: 1.0 \n",
    "\n",
    "Dataset # 2\n",
    "\n",
    "- Decision Tree: test set score: <b>0.775</b>  (old 0.65)\n",
    "- Decision Tree: train set score: <b>1.0</b> \n",
    "\n",
    "\n",
    "- Random Forest: test set score: <b>0.84875</b>  (old 75)\n",
    "- Random Forest: train set score: <b>0.985</b>  ( old 96)\n",
    "\n",
    "\n",
    "Overfitting has dropped a little\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So what if too much noise is increased, lets check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------\n",
    "- n_samples = 200\n",
    "- n_features = 10\n",
    "- n_classes = 2\n",
    "- n_estimators = 10\n",
    "- noise_moon = 0.1 noise_circle = 0.1 noise_class = 0.2\n",
    "\n",
    "------------------------------\n",
    "\n",
    "## Results :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Dataset # 0</b>\n",
    "\n",
    "- Decision Tree: test set score: 0.72125 \n",
    "- Decision Tree: train set score: 1.0 \n",
    "\n",
    "        \n",
    "- Random Forest: test set score: 0.76125 \n",
    "- Random Forest: train set score: 0.9791666666666666 \n",
    "\n",
    "<b>Dataset # 1</b>\n",
    "\n",
    "- Decision Tree: test set score: 0.575 \n",
    "- Decision Tree: train set score: 1.0 \n",
    "\n",
    "        \n",
    "- Random Forest: test set score: 0.6425 \n",
    "- Random Forest: train set score: 0.9816666666666667 \n",
    "\n",
    "<b>Dataset # 2</b>\n",
    "\n",
    "- Decision Tree: test set score: 0.5175 \n",
    "- Decision Tree: train set score: 1.0 \n",
    "\n",
    "        \n",
    "- Random Forest: test set score: 0.52875 \n",
    "- Random Forest: train set score: 0.9816666666666667 \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>We couldn't decreased the overfitting with this way</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> change max_depth to 2 </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------\n",
    "- n_samples = 200\n",
    "- n_features = 10\n",
    "- n_classes = 2\n",
    "- n_estimators = 10\n",
    "- noise_moon = 0.1 noise_circle = 0.1 noise_class = 0.2\n",
    "\n",
    "------------------------------\n",
    "\n",
    "## Results :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Dataset # 0</b>\n",
    "\n",
    "- Decision Tree: test set score: 0.78875 \n",
    "- Decision Tree: train set score: 0.8108333333333333 \n",
    "\n",
    "\n",
    "- Random Forest: test set score: 0.785 \n",
    "- Random Forest: train set score: 0.8016666666666666 \n",
    "\n",
    "<b>Dataset # 1</b>\n",
    "\n",
    "- Decision Tree: test set score: 0.58875 \n",
    "- Decision Tree: train set score: 0.6241666666666666 \n",
    "\n",
    "\n",
    "- Random Forest: test set score: 0.6475 \n",
    "- Random Forest: train set score: 0.705 \n",
    "\n",
    "<b>Dataset # 2</b>\n",
    "\n",
    "- Decision Tree: test set score: 0.585 \n",
    "- Decision Tree: train set score: 0.6108333333333333 \n",
    "\n",
    "\n",
    "- Random Forest: test set score: <b>0.58875</b> ( old 52)\n",
    "- Random Forest: train set score: <b>0.6241666666666666</b> (old 98) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>We can say fix the overfitting problem but our test score dropped considerably in general</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Difference Between Random Forest and Adaboosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Random Forest is based on bagging technique while Adaboost is based on boosting technique.\n",
    "\n",
    "2. In Random Forest, certain number of full sized trees are grown on different subsets of the training dataset. Adaboost uses stumps (decision tree with only one split). So, Adaboost is basically a forest of stumps. These stumps are called weak learners. These weak learners have high bias and low variance. \n",
    "\n",
    "3. Each tree in the Random Forest is made up using all the features in the dataset while stumps use one feature at a time.\n",
    "\n",
    "4. In Random Forest, each decision tree is made independent of each other. So, the order in which trees are made is not important. While in Adaboost, order of stumps do matter. The error that first stump makes, influence how the second stump is made and so on. Each stump is made by taking the previous stump's mistakes into account. It takes the errors from the first round of predictions, and passes the errors as a new target to the second stump. The second stump will model the error from the first stump, record the new errors and pass that as a target to the third stump. And so forth. Essentially, it focuses on modelling errors from previous stumps.\n",
    "\n",
    "5. Random Forest uses parallel ensembling while Adaboost uses sequential ensembling. Random Forest runs trees in parallel, thus making it possible to parallelize jobs on a multiprocessor machine. Adaboost instead uses a sequential approach. \n",
    "\n",
    "6. Each tree in the Random Forest has equal amount of say in the final decision while in Adaboost different stumps have different amount of say in the final decision. The stump which makes less error in the prediction, has high amount of say as compared to the stump which makes more errors.\n",
    "\n",
    "7. Random Forest aims to <b>decrease variance</b> not bias while Adaboost aims to <b>decrease bias</b> not variance.\n",
    "\n",
    "8. There are <b>rare</b> chances of Random Forest to <b>overfit</b> while there are <b>good</b> chances of Adaboost to <b>overfit</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
